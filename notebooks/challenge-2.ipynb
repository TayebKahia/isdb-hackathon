{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca540fc",
   "metadata": {},
   "source": [
    "# Challenge 2: Reverse Transactions Analyzer\n",
    "\n",
    "This notebook implements an AI solution for the \"Reverse Transactions\" category of the \"Strengthening the Adoption of Standards in Islamic Finance with Artificial Intelligence\" Hackathon.\n",
    "\n",
    "## Objective\n",
    "Given \"out-of-context\" financial entries (journal entries and brief context), our AI solution will identify the relevant AAOIFI Financial Accounting Standard(s) (FAS) that govern such transactions. If multiple FAS are possible, the system provides a weighted probability and reasoning.\n",
    "\n",
    "## Approach\n",
    "1. Process and index the AAOIFI standards (FAS and SS) using RAG techniques\n",
    "2. Create a specialized prompt engineering system for reverse transaction analysis \n",
    "3. Provide weighted probability estimations for applicable standards\n",
    "4. Justify the analysis with references to specific sections of the standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d0a97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb86b41",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction from FAS and SS Standards\n",
    "First, we'll extract text from the relevant AAOIFI standards documents (FAS and SS). These documents contain the rules and guidelines that determine the appropriate accounting treatments for various Islamic finance transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf560b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to extract text from PDF documents\n",
    "# def extract_pdf_text(pdf_path):\n",
    "#     text = \"\"\n",
    "#     try:\n",
    "#         with pdfplumber.open(pdf_path) as pdf:\n",
    "#             for page in pdf.pages:\n",
    "#                 page_text = page.extract_text()\n",
    "#                 if page_text:\n",
    "#                     text += page_text + \"\\n\"\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # Get all PDF files from the data directory\n",
    "# data_dir = \"../data\"\n",
    "# pdf_files = [f for f in os.listdir(data_dir) if f.endswith('.pdf') or f.endswith('.PDF')]\n",
    "\n",
    "# # Extract text from each PDF\n",
    "# documents = []\n",
    "# for pdf_file in pdf_files:\n",
    "#     pdf_path = os.path.join(data_dir, pdf_file)\n",
    "#     text = extract_pdf_text(pdf_path)\n",
    "#     if text:\n",
    "#         documents.append({\n",
    "#             \"source\": pdf_file,\n",
    "#             \"text\": text\n",
    "#         })\n",
    "#         print(f\"Successfully extracted text from {pdf_file}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to extract text from {pdf_file}\")\n",
    "\n",
    "# print(f\"\\nTotal documents processed: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f959d",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing and Chunking\n",
    "To optimize the retrieval process, we need to divide the document text into smaller, meaningful chunks. This enables more precise matching and retrieval when handling specific queries about accounting standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5472c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize text splitter for chunking\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,  # Larger chunks to capture more context\n",
    "#     chunk_overlap=200,  # Significant overlap to preserve context across chunks\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "# # Process documents and create chunks with metadata\n",
    "# chunks = []\n",
    "# for doc in documents:\n",
    "#     for i, chunk in enumerate(splitter.split_text(doc[\"text\"])):\n",
    "#         # Extract standard number from filename if possible\n",
    "#         standard_match = re.search(r'(FAS|SS)[\\s_-]*(\\d+)', doc[\"source\"], re.IGNORECASE)\n",
    "#         standard_type = standard_match.group(1).upper() if standard_match else \"Unknown\"\n",
    "#         standard_number = standard_match.group(2) if standard_match else \"Unknown\"\n",
    "        \n",
    "#         chunks.append({\n",
    "#             \"source\": doc[\"source\"],\n",
    "#             \"text\": chunk,\n",
    "#             \"chunk_id\": i,\n",
    "#             \"standard_type\": standard_type,\n",
    "#             \"standard_number\": standard_number\n",
    "#         })\n",
    "\n",
    "# print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0802f6",
   "metadata": {},
   "source": [
    "## Step 3: Building the Vector Database\n",
    "Now we'll create a vector database to store and retrieve our document chunks. This enables semantic search capabilities, allowing us to find the most relevant standard sections for a given financial transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78a7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define path for Chroma persistence\n",
    "# CHROMA_PATH = \"../vector_db/standards_reverse_transactions\"\n",
    "\n",
    "# # Initialize OpenAI embeddings\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# # Extract chunk texts and prepare metadata\n",
    "# chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "# chunk_metadatas = [\n",
    "#     {\n",
    "#         \"source\": chunk[\"source\"],\n",
    "#         \"chunk_id\": chunk[\"chunk_id\"],\n",
    "#         \"standard_type\": chunk[\"standard_type\"],\n",
    "#         \"standard_number\": chunk[\"standard_number\"]\n",
    "#     } \n",
    "#     for chunk in chunks\n",
    "# ]\n",
    "\n",
    "# # Check if the vector store already exists\n",
    "# if os.path.exists(CHROMA_PATH):\n",
    "#     # Load existing vector store\n",
    "#     print(\"Loading existing Chroma vector store...\")\n",
    "#     vector_store = Chroma(\n",
    "#         persist_directory=CHROMA_PATH,\n",
    "#         embedding_function=embeddings\n",
    "#     )\n",
    "# else:\n",
    "#     # Create a new vector store\n",
    "#     print(\"Creating new Chroma vector store...\")\n",
    "#     vector_store = Chroma.from_texts(\n",
    "#         texts=chunk_texts,\n",
    "#         embedding=embeddings,\n",
    "#         metadatas=chunk_metadatas,\n",
    "#         persist_directory=CHROMA_PATH\n",
    "#     )\n",
    "#     # Persist the vector store to disk\n",
    "#     vector_store.persist()\n",
    "#     print(f\"Saved vector store to {CHROMA_PATH}\")\n",
    "    \n",
    "# print(f\"Vector store contains {vector_store._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44203eb1",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval Functions\n",
    "Let's create functions to retrieve the most relevant standards for a given financial transaction. These functions will help us find the right context to determine which FAS applies to a particular journal entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8996f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_standards(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant chunks from our vector store for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The text query about a financial transaction\n",
    "        top_k (int): Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks with metadata\n",
    "    \"\"\"\n",
    "    # Use Chroma's similarity search to find relevant chunks\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Convert the returned documents to our expected format\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        results.append({\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"standard_type\": doc.metadata[\"standard_type\"],\n",
    "            \"standard_number\": doc.metadata[\"standard_number\"],\n",
    "            \"text\": doc.page_content\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def filter_by_standard(results, standard_type=None, standard_number=None):\n",
    "    \"\"\"\n",
    "    Filter retrieved results by standard type and/or number.\n",
    "    \n",
    "    Args:\n",
    "        results (list): List of document chunks\n",
    "        standard_type (str, optional): Filter by standard type (FAS, SS)\n",
    "        standard_number (str, optional): Filter by standard number\n",
    "        \n",
    "    Returns:\n",
    "        list: Filtered list of document chunks\n",
    "    \"\"\"\n",
    "    filtered = results\n",
    "    \n",
    "    if standard_type:\n",
    "        filtered = [r for r in filtered if r[\"standard_type\"].upper() == standard_type.upper()]\n",
    "    \n",
    "    if standard_number:\n",
    "        filtered = [r for r in filtered if r[\"standard_number\"] == standard_number]\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881316ce",
   "metadata": {},
   "source": [
    "## Step 5: FAS Identification and Weighting\n",
    "The core of our solution is the ability to identify which AAOIFI standards are most relevant to a given financial transaction. We'll use an LLM to analyze the transaction details and provide weighted probabilities for each potentially applicable standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5347cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transaction(transaction_description, journal_entry=None, top_k=7):\n",
    "    \"\"\"\n",
    "    Analyze a financial transaction and identify relevant AAOIFI standards.\n",
    "    \n",
    "    Args:\n",
    "        transaction_description (str): Description of the transaction\n",
    "        journal_entry (str, optional): Journal entry related to the transaction\n",
    "        top_k (int): Number of relevant chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results with weighted probabilities\n",
    "    \"\"\"\n",
    "    # Construct a comprehensive query combining description and journal entry\n",
    "    query = transaction_description\n",
    "    if journal_entry:\n",
    "        query += f\"\\n{journal_entry}\"\n",
    "    \n",
    "    # Enhance the query with accounting terminology to improve retrieval\n",
    "    enhanced_query = f\"\"\"\n",
    "    Financial transaction analysis:\n",
    "    {query}\n",
    "    \n",
    "    Relevant AAOIFI standards, accounting treatments, and financial instruments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_standards(enhanced_query, top_k=top_k)\n",
    "    \n",
    "    # Filter to focus on FAS documents\n",
    "    fas_chunks = filter_by_standard(relevant_chunks, standard_type=\"FAS\")\n",
    "    \n",
    "    # Extract unique FAS numbers for consideration\n",
    "    potential_standards = []\n",
    "    seen_standards = set()\n",
    "    \n",
    "    for chunk in fas_chunks:\n",
    "        standard_id = f\"FAS {chunk['standard_number']}\"\n",
    "        if standard_id not in seen_standards and chunk['standard_number'] != \"Unknown\":\n",
    "            seen_standards.add(standard_id)\n",
    "            potential_standards.append({\n",
    "                \"id\": standard_id,\n",
    "                \"context\": chunk['text'][:500]  # Brief context from the standard\n",
    "            })\n",
    "    \n",
    "    # Also include SS standards as they often contain complementary guidance\n",
    "    ss_chunks = filter_by_standard(relevant_chunks, standard_type=\"SS\")\n",
    "    ss_references = []\n",
    "    \n",
    "    for chunk in ss_chunks:\n",
    "        standard_id = f\"SS {chunk['standard_number']}\"\n",
    "        if standard_id not in seen_standards and chunk['standard_number'] != \"Unknown\":\n",
    "            seen_standards.add(standard_id)\n",
    "            ss_references.append({\n",
    "                \"id\": standard_id,\n",
    "                \"context\": chunk['text'][:300]  # Brief context from the standard\n",
    "            })\n",
    "    \n",
    "    # Combine all relevant contexts for the LLM\n",
    "    context_text = \"\"\n",
    "    for std in potential_standards:\n",
    "        context_text += f\"Standard {std['id']}:\\n{std['context']}\\n\\n\"\n",
    "    \n",
    "    for std in ss_references:\n",
    "        context_text += f\"Supporting standard {std['id']}:\\n{std['context']}\\n\\n\"\n",
    "    \n",
    "    # Create LLM prompt to analyze and weight the standards\n",
    "    llm = OpenAI(temperature=0)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"transaction\", \"journal_entry\", \"context\", \"standard_ids\"],\n",
    "        template=\"\"\"\n",
    "You are an expert in Islamic finance and AAOIFI standards. Given a financial transaction and relevant excerpts from AAOIFI standards, analyze which standards are most applicable.\n",
    "\n",
    "Transaction description:\n",
    "{transaction}\n",
    "\n",
    "Journal entry (if provided):\n",
    "{journal_entry}\n",
    "\n",
    "Excerpts from potentially relevant standards:\n",
    "{context}\n",
    "\n",
    "Potential standards to consider: {standard_ids}\n",
    "\n",
    "Based on your analysis, provide the following in JSON format:\n",
    "1. The FAS standards that apply to this transaction, with probability weights (0-100) totaling 100%\n",
    "2. A brief reasoning for each standard's applicability\n",
    "3. A determination if the journal entry appears to comply with the identified standards\n",
    "4. Any relevant Shariah considerations from the SS standards\n",
    "\n",
    "Return only the JSON object without additional commentary. Example format:\n",
    "{{\n",
    "  \"applicable_standards\": [\n",
    "    {{\n",
    "      \"standard\": \"FAS 4\",\n",
    "      \"probability\": 70,\n",
    "      \"reasoning\": \"This standard applies because...\"\n",
    "    }},\n",
    "    {{\n",
    "      \"standard\": \"FAS 10\",\n",
    "      \"probability\": 30,\n",
    "      \"reasoning\": \"This standard is somewhat relevant because...\"\n",
    "    }}\n",
    "  ],\n",
    "  \"compliance_assessment\": \"The journal entry appears to comply with FAS 28 because...\",\n",
    "  \"shariah_considerations\": \"According to SS 9, this transaction should also consider...\"\n",
    "}}\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Prepare standard IDs for the prompt\n",
    "    standard_ids = \", \".join([std[\"id\"] for std in potential_standards])\n",
    "    \n",
    "    # Run the analysis\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.run(\n",
    "        transaction=transaction_description,\n",
    "        journal_entry=journal_entry if journal_entry else \"No journal entry provided\",\n",
    "        context=context_text,\n",
    "        standard_ids=standard_ids\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON output\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, try to extract JSON from the response\n",
    "        match = re.search(r'({.*})', response, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Return error if parsing fails\n",
    "        return {\n",
    "            \"error\": \"Failed to parse LLM response\",\n",
    "            \"raw_response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b8451",
   "metadata": {},
   "source": [
    "## Step 6: Testing with Example Cases\n",
    "Let's test our implementation with the example cases provided in the hackathon challenge:\n",
    "\n",
    "### Example 1: GreenTech Exit and Buyout\n",
    "- Context: GreenTech exits in Year 3, and Al Baraka Bank buys out its stake\n",
    "- Adjustments: Buyout Price: $1,750,000; Bank Ownership: 100%\n",
    "- Journal Entry: Dr. GreenTech Equity $1,750,000 / Cr. Cash $1,750,000\n",
    "\n",
    "### Example 2: Contract Reversal\n",
    "- Context: Client cancels a change order, reverting to original contract terms\n",
    "- Adjustments: Revised Contract Value back to $5,000,000; Timeline Restored: 2 years\n",
    "- Journal Entry: Dr. Accounts Payable $1,000,000 / Cr. Work-in-Progress $1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982b259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Test Case 1: GreenTech Exit and Buyout\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnalyzing Test Case 1: GreenTech Exit and Buyout\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m result_1 = \u001b[43manalyze_transaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case_1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdescription\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_case_1\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjournal_entry\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result_1, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36manalyze_transaction\u001b[39m\u001b[34m(transaction_description, journal_entry, top_k)\u001b[39m\n\u001b[32m     19\u001b[39m enhanced_query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33mFinancial transaction analysis:\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33mRelevant AAOIFI standards, accounting treatments, and financial instruments\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Retrieve relevant chunks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m relevant_chunks = \u001b[43mretrieve_relevant_standards\u001b[49m\u001b[43m(\u001b[49m\u001b[43menhanced_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Filter to focus on FAS documents\u001b[39;00m\n\u001b[32m     30\u001b[39m fas_chunks = filter_by_standard(relevant_chunks, standard_type=\u001b[33m\"\u001b[39m\u001b[33mFAS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mretrieve_relevant_standards\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mRetrieve the most relevant chunks from our vector store for a given query.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    list: List of document chunks with metadata\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Use Chroma's similarity search to find relevant chunks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m docs = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert the returned documents to our expected format\u001b[39;00m\n\u001b[32m     16\u001b[39m results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:350\u001b[39m, in \u001b[36mChroma.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    335\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    338\u001b[39m     **kwargs: Any,\n\u001b[32m    339\u001b[39m ) -> List[Document]:\n\u001b[32m    340\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[32m    341\u001b[39m \n\u001b[32m    342\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m \u001b[33;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:439\u001b[39m, in \u001b[36mChroma.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, where_document, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.__query_collection(\n\u001b[32m    432\u001b[39m         query_texts=[query],\n\u001b[32m    433\u001b[39m         n_results=k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    436\u001b[39m         **kwargs,\n\u001b[32m    437\u001b[39m     )\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     query_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.__query_collection(\n\u001b[32m    441\u001b[39m         query_embeddings=[query_embedding],\n\u001b[32m    442\u001b[39m         n_results=k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m         **kwargs,\n\u001b[32m    446\u001b[39m     )\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:620\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    613\u001b[39m \n\u001b[32m    614\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m \u001b[33;03m        Embedding for the text.\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:575\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    574\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:471\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size)\u001b[39m\n\u001b[32m    469\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invocation_params\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    475\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\openai\\resources\\embeddings.py:128\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    122\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    123\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m             ).tolist()\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\course-env\\Lib\\site-packages\\openai\\_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Test Case 1: GreenTech Exit and Buyout\n",
    "test_case_1 = {\n",
    "    \"description\": \"\"\"\n",
    "    GreenTech exits in Year 3, and Al Baraka Bank buys out its stake. \n",
    "    After this buyout, the bank's ownership becomes 100%. \n",
    "    The accounting treatment involves derecognition of GreenTech's equity and recognition of the acquisition expense.\n",
    "    \"\"\",\n",
    "    \"journal_entry\": \"Dr. GreenTech Equity $1,750,000 / Cr. Cash $1,750,000\"\n",
    "}\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Analyzing Test Case 1: GreenTech Exit and Buyout\\n\")\n",
    "result_1 = analyze_transaction(test_case_1[\"description\"], test_case_1[\"journal_entry\"])\n",
    "\n",
    "# Display the results\n",
    "print(json.dumps(result_1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Test Case 2: Contract Reversal\n",
      "\n",
      "{\n",
      "  \"applicable_standards\": [\n",
      "    {\n",
      "      \"standard\": \"FAS 28\",\n",
      "      \"probability\": 80,\n",
      "      \"reasoning\": \"This standard applies because it deals with accounting for changes in contract terms and the cancellation of a change order would require adjustments to revenue and cost projections.\"\n",
      "    },\n",
      "    {\n",
      "      \"standard\": \"FAS 30\",\n",
      "      \"probability\": 20,\n",
      "      \"reasoning\": \"This standard is somewhat relevant because it deals with accounting for revenue and cost adjustments, which would be necessary in this transaction.\"\n",
      "    }\n",
      "  ],\n",
      "  \"compliance_assessment\": \"The journal entry appears to comply with FAS 28 because it correctly debits Accounts Payable and credits Work-in-Progress, reflecting the cancellation of the change order and the reversion to the original contract terms.\",\n",
      "  \"shariah_considerations\": \"According to SS 9, this transaction should also consider the principles of fairness and transparency in the cancellation of the change order and the restoration of the original contract terms.\"\n",
      "}\n",
      "{\n",
      "  \"applicable_standards\": [\n",
      "    {\n",
      "      \"standard\": \"FAS 28\",\n",
      "      \"probability\": 80,\n",
      "      \"reasoning\": \"This standard applies because it deals with accounting for changes in contract terms and the cancellation of a change order would require adjustments to revenue and cost projections.\"\n",
      "    },\n",
      "    {\n",
      "      \"standard\": \"FAS 30\",\n",
      "      \"probability\": 20,\n",
      "      \"reasoning\": \"This standard is somewhat relevant because it deals with accounting for revenue and cost adjustments, which would be necessary in this transaction.\"\n",
      "    }\n",
      "  ],\n",
      "  \"compliance_assessment\": \"The journal entry appears to comply with FAS 28 because it correctly debits Accounts Payable and credits Work-in-Progress, reflecting the cancellation of the change order and the reversion to the original contract terms.\",\n",
      "  \"shariah_considerations\": \"According to SS 9, this transaction should also consider the principles of fairness and transparency in the cancellation of the change order and the restoration of the original contract terms.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Contract Reversal\n",
    "test_case_2 = {\n",
    "    \"description\": \"\"\"\n",
    "    The client decided to cancel a change order that had previously modified the contract terms. \n",
    "    This cancellation reverts all terms back to the original contract. \n",
    "    The contract value is revised back to $5,000,000, and the project timeline is restored to the original 2 years. \n",
    "    This requires adjustment of revenue and cost projections, as well as a reversal of additional cost accruals.\n",
    "    \"\"\",\n",
    "    \"journal_entry\": \"Dr. Accounts Payable $1,000,000 / Cr. Work-in-Progress $1,000,000\"\n",
    "}\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Analyzing Test Case 2: Contract Reversal\\n\")\n",
    "result_2 = analyze_transaction(test_case_2[\"description\"], test_case_2[\"journal_entry\"])\n",
    "\n",
    "# Display the results\n",
    "print(json.dumps(result_2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a54c6e",
   "metadata": {},
   "source": [
    "## Step 7: Feedback and Correction Mechanism\n",
    "A critical feature of our solution is the ability to learn from feedback. When our system's prediction differs from the correct answer, we can analyze the discrepancy and improve future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_with_correct_answer(analysis_result, correct_standards, transaction_data):\n",
    "    \"\"\"\n",
    "    Compare our analysis with the correct answer and provide reconciliation insights.\n",
    "    \n",
    "    Args:\n",
    "        analysis_result (dict): Our system's analysis result\n",
    "        correct_standards (list): List of correct standards in priority order\n",
    "        transaction_data (dict): Original transaction data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Reconciliation analysis with improvement insights\n",
    "    \"\"\"\n",
    "    llm = OpenAI(temperature=0)\n",
    "    \n",
    "    # Format our results and the correct answer\n",
    "    our_standards = [item[\"standard\"] for item in analysis_result[\"applicable_standards\"]]\n",
    "    our_analysis = json.dumps(analysis_result, indent=2)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"transaction\", \"journal_entry\", \"our_analysis\", \"our_standards\", \"correct_standards\"],\n",
    "        template=\"\"\"\n",
    "You are an expert Islamic finance compliance advisor. Compare our system's analysis of a transaction with the correct standards and explain any discrepancies.\n",
    "\n",
    "Transaction description:\n",
    "{transaction}\n",
    "\n",
    "Journal entry:\n",
    "{journal_entry}\n",
    "\n",
    "Our system's analysis:\n",
    "{our_analysis}\n",
    "\n",
    "Our predicted standards: {our_standards}\n",
    "Correct standards (in priority order): {correct_standards}\n",
    "\n",
    "Provide a detailed analysis addressing the following in JSON format:\n",
    "1. Whether our prediction matched the correct standards (full match, partial match, or no match)\n",
    "2. For each missed or incorrectly weighted standard, explain why it should have been identified\n",
    "3. Key patterns in the transaction text and journal entry that indicate the correct standards\n",
    "4. Specific contextual clues our system may have missed\n",
    "5. How our analysis should be improved in the future\n",
    "\n",
    "Return only the JSON object without additional commentary.\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Run the reconciliation analysis\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.run(\n",
    "        transaction=transaction_data[\"description\"],\n",
    "        journal_entry=transaction_data[\"journal_entry\"],\n",
    "        our_analysis=our_analysis,\n",
    "        our_standards=\", \".join(our_standards),\n",
    "        correct_standards=\", \".join(correct_standards)\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON output\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        # If parsing fails, try to extract JSON from the response\n",
    "        match = re.search(r'({.*})', response, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Return error if parsing fails\n",
    "        return {\n",
    "            \"error\": \"Failed to parse LLM response\",\n",
    "            \"raw_response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconciliation Analysis for Test Case 1:\n",
      "\n",
      "{\n",
      "  \"error\": \"Failed to parse LLM response\",\n",
      "  \"raw_response\": \"\\n{\\n  \\\"prediction_match\\\": \\\"partial match\\\",\\n  \\\"missed_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 4\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for investments in subsidiaries, because it only focused on associates and joint ventures.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 20\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for equity method investments, because it only focused on associates and joint ventures.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 32\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for business combinations, because it only focused on the acquisition of associates and joint ventures.\\\"\\n    }\\n  ],\\n  \\\"key_patterns\\\": [\\n    \\\"buyout\\\",\\n    \\\"ownership\\\",\\n    \\\"acquisition expense\\\"\\n  ],\\n  \\\"contextual_clues\\\": [\\n    \\\"100% ownership\\\",\\n    \\\"derecognition of equity\\\"\\n  ],\\n  \\\"improvement_suggestions\\\": [\\n    \\\"Expand the scope of standards to include subsidiaries and equity method investments\\\",\\n    \\\"Include keywords related to business combinations in the analysis\\\",\\n    \\\"Consider\"\n",
      "}\n",
      "\n",
      "Reconciliation Analysis for Test Case 2:\n",
      "\n",
      "{\n",
      "  \"error\": \"Failed to parse LLM response\",\n",
      "  \"raw_response\": \"\\n{\\n  \\\"prediction_match\\\": \\\"partial match\\\",\\n  \\\"missed_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 4\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for investments in subsidiaries, because it only focused on associates and joint ventures.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 20\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for equity method investments, because it only focused on associates and joint ventures.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 32\\\",\\n      \\\"reasoning\\\": \\\"Our system did not identify this standard, which deals with the accounting treatment for business combinations, because it only focused on the acquisition of associates and joint ventures.\\\"\\n    }\\n  ],\\n  \\\"key_patterns\\\": [\\n    \\\"buyout\\\",\\n    \\\"ownership\\\",\\n    \\\"acquisition expense\\\"\\n  ],\\n  \\\"contextual_clues\\\": [\\n    \\\"100% ownership\\\",\\n    \\\"derecognition of equity\\\"\\n  ],\\n  \\\"improvement_suggestions\\\": [\\n    \\\"Expand the scope of standards to include subsidiaries and equity method investments\\\",\\n    \\\"Include keywords related to business combinations in the analysis\\\",\\n    \\\"Consider\"\n",
      "}\n",
      "\n",
      "Reconciliation Analysis for Test Case 2:\n",
      "\n",
      "{\n",
      "  \"error\": \"Failed to parse LLM response\",\n",
      "  \"raw_response\": \"\\n{\\n  \\\"prediction_match\\\": \\\"partial match\\\",\\n  \\\"missed_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 10\\\",\\n      \\\"reasoning\\\": \\\"Our system missed this standard because it deals with accounting for changes in contract terms and the cancellation of a change order, which is the main focus of this transaction.\\\"\\n    }\\n  ],\\n  \\\"incorrectly_weighted_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 30\\\",\\n      \\\"reasoning\\\": \\\"Our system incorrectly weighted this standard because it is only somewhat relevant to this transaction, as it deals with revenue and cost adjustments rather than the cancellation of a change order.\\\"\\n    }\\n  ],\\n  \\\"key_patterns\\\": [\\n    \\\"cancellation of a change order\\\",\\n    \\\"reversion to original contract terms\\\",\\n    \\\"adjustments to revenue and cost projections\\\"\\n  ],\\n  \\\"contextual_clues\\\": [\\n    \\\"SS 9\\\",\\n    \\\"principles of fairness and transparency\\\"\\n  ],\\n  \\\"improvement_suggestions\\\": [\\n    \\\"Our system should have a more comprehensive understanding of the specific standards that apply to Islamic finance compliance, such as FAS 10 in this case.\\\",\\n    \\\"Our system should also consider the context and principles of Islamic finance, such\"\n",
      "}\n",
      "{\n",
      "  \"error\": \"Failed to parse LLM response\",\n",
      "  \"raw_response\": \"\\n{\\n  \\\"prediction_match\\\": \\\"partial match\\\",\\n  \\\"missed_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 10\\\",\\n      \\\"reasoning\\\": \\\"Our system missed this standard because it deals with accounting for changes in contract terms and the cancellation of a change order, which is the main focus of this transaction.\\\"\\n    }\\n  ],\\n  \\\"incorrectly_weighted_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 30\\\",\\n      \\\"reasoning\\\": \\\"Our system incorrectly weighted this standard because it is only somewhat relevant to this transaction, as it deals with revenue and cost adjustments rather than the cancellation of a change order.\\\"\\n    }\\n  ],\\n  \\\"key_patterns\\\": [\\n    \\\"cancellation of a change order\\\",\\n    \\\"reversion to original contract terms\\\",\\n    \\\"adjustments to revenue and cost projections\\\"\\n  ],\\n  \\\"contextual_clues\\\": [\\n    \\\"SS 9\\\",\\n    \\\"principles of fairness and transparency\\\"\\n  ],\\n  \\\"improvement_suggestions\\\": [\\n    \\\"Our system should have a more comprehensive understanding of the specific standards that apply to Islamic finance compliance, such as FAS 10 in this case.\\\",\\n    \\\"Our system should also consider the context and principles of Islamic finance, such\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Correct answers from the challenge description\n",
    "correct_answer_1 = [\"FAS 4\", \"FAS 20\", \"FAS 32\"]  # In priority order\n",
    "correct_answer_2 = [\"FAS 10\"]  # In priority order\n",
    "\n",
    "# Reconcile our predictions with correct answers\n",
    "print(\"Reconciliation Analysis for Test Case 1:\\n\")\n",
    "reconciliation_1 = reconcile_with_correct_answer(result_1, correct_answer_1, test_case_1)\n",
    "print(json.dumps(reconciliation_1, indent=2))\n",
    "\n",
    "print(\"\\nReconciliation Analysis for Test Case 2:\\n\")\n",
    "reconciliation_2 = reconcile_with_correct_answer(result_2, correct_answer_2, test_case_2)\n",
    "print(json.dumps(reconciliation_2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad92d2",
   "metadata": {},
   "source": [
    "## Step 8: Building a Streamlined User Interface Function\n",
    "Let's create a simplified user interface function that allows users to input transaction details and receive a comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_financial_transaction(description, journal_entry=None, correct_standards=None):\n",
    "    \"\"\"\n",
    "    Comprehensive function to analyze a financial transaction and identify relevant AAOIFI standards.\n",
    "    \n",
    "    Args:\n",
    "        description (str): Description of the transaction\n",
    "        journal_entry (str, optional): Journal entry related to the transaction\n",
    "        correct_standards (list, optional): List of correct standards for feedback\n",
    "        \n",
    "    Returns:\n",
    "        dict: Complete analysis results\n",
    "    \"\"\"\n",
    "    # Step 1: Analyze the transaction\n",
    "    analysis = analyze_transaction(description, journal_entry)\n",
    "    \n",
    "    # Create a formatted output\n",
    "    result = {\n",
    "        \"input\": {\n",
    "            \"description\": description,\n",
    "            \"journal_entry\": journal_entry\n",
    "        },\n",
    "        \"analysis\": analysis\n",
    "    }\n",
    "    \n",
    "    # Step 2: If correct standards are provided, add reconciliation\n",
    "    if correct_standards:\n",
    "        reconciliation = reconcile_with_correct_answer(\n",
    "            analysis, \n",
    "            correct_standards, \n",
    "            {\"description\": description, \"journal_entry\": journal_entry}\n",
    "        )\n",
    "        result[\"reconciliation\"] = reconciliation\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90228f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Musharakah Transaction:\n",
      "\n",
      "{\n",
      "  \"input\": {\n",
      "    \"description\": \"\\n    On 1 January 2023, Islamic Bank A (the Bank) entered into a Musharakah agreement with \\n    Company B to finance a commercial real estate development project. The Bank contributed \\n    60% (USD 6 million) while Company B contributed 40% (USD 4 million) of the total project \\n    cost of USD 10 million. Company B will manage the project. Profits will be shared 50:50, \\n    while losses will be borne in proportion to capital contribution.\\n\\n    On 30 June 2023, after construction delays and rising costs, the partners agreed to amend \\n    the agreement. The Bank contributed an additional USD 2 million, increasing its partnership \\n    share to 67%. The profit-sharing ratio was adjusted to 60:40 in favor of the Bank.\\n    \",\n",
      "    \"journal_entry\": \"Dr. Musharakah Investment (Company B Project) $2,000,000 / Cr. Cash $2,000,000\"\n",
      "  },\n",
      "  \"analysis\": {\n",
      "    \"error\": \"Failed to parse LLM response\",\n",
      "    \"raw_response\": \"\\n{\\n  \\\"applicable_standards\\\": [\\n    {\\n      \\\"standard\\\": \\\"FAS 28\\\",\\n      \\\"probability\\\": 80,\\n      \\\"reasoning\\\": \\\"This standard applies because it deals with accounting for investments in partnerships, which is relevant to the Musharakah agreement between the Bank and Company B.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 30\\\",\\n      \\\"probability\\\": 70,\\n      \\\"reasoning\\\": \\\"This standard applies because it deals with accounting for the effects of changes in foreign exchange rates, which may be relevant to the additional contribution made by the Bank in USD.\\\"\\n    },\\n    {\\n      \\\"standard\\\": \\\"FAS 31\\\",\\n      \\\"probability\\\": 50,\\n      \\\"reasoning\\\": \\\"This standard applies because it deals with accounting for joint ventures, which is relevant to the partnership between the Bank and Company B in the commercial real estate development project.\\\"\\n    }\\n  ],\\n  \\\"compliance_assessment\\\": \\\"The journal entry appears to comply with FAS 28, FAS 30, and FAS 31 because it properly records the additional contribution made by the Bank and the corresponding increase in its partnership share.\\\",\\n  \\\"shariah_considerations\\\": \\\"According to SS 9, this transaction should\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage of our streamlined interface\n",
    "new_transaction = {\n",
    "    \"description\": \"\"\"\n",
    "    On 1 January 2023, Islamic Bank A (the Bank) entered into a Musharakah agreement with \n",
    "    Company B to finance a commercial real estate development project. The Bank contributed \n",
    "    60% (USD 6 million) while Company B contributed 40% (USD 4 million) of the total project \n",
    "    cost of USD 10 million. Company B will manage the project. Profits will be shared 50:50, \n",
    "    while losses will be borne in proportion to capital contribution.\n",
    "    \n",
    "    On 30 June 2023, after construction delays and rising costs, the partners agreed to amend \n",
    "    the agreement. The Bank contributed an additional USD 2 million, increasing its partnership \n",
    "    share to 67%. The profit-sharing ratio was adjusted to 60:40 in favor of the Bank.\n",
    "    \"\"\",\n",
    "    \"journal_entry\": \"Dr. Musharakah Investment (Company B Project) $2,000,000 / Cr. Cash $2,000,000\"\n",
    "}\n",
    "\n",
    "# Run the analysis with our unified function\n",
    "musharakah_analysis = analyze_financial_transaction(\n",
    "    new_transaction[\"description\"], \n",
    "    new_transaction[\"journal_entry\"]\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Analysis of Musharakah Transaction:\\n\")\n",
    "print(json.dumps(musharakah_analysis, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e74f72",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates an AI-based approach to analyzing financial transactions in the context of Islamic finance, particularly addressing the \"Reverse Transactions\" challenge. Our solution:\n",
    "\n",
    "1. Processes and indexes AAOIFI standards documents to create a knowledge base\n",
    "2. Retrieves relevant standard excerpts for a given transaction\n",
    "3. Analyzes transactions to identify applicable standards with weighted probabilities\n",
    "4. Provides reasoning for each standard's applicability\n",
    "5. Compares predictions with correct answers and provides reconciliation insights\n",
    "6. Offers a streamlined interface for transaction analysis\n",
    "\n",
    "This approach can be further enhanced by:\n",
    "1. Incorporating more detailed metadata extraction from the standards\n",
    "2. Implementing a fine-tuned model specifically for Islamic finance transaction classification\n",
    "3. Expanding the knowledge base with additional interpretive guidance and scholarly opinions\n",
    "4. Adding an interactive feedback loop to continuously improve the system's accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
